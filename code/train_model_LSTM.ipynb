{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Bidirectional\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mainfs/lyceum/ao2u17/Dissertation\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "print(path)\n",
    "filename = '/encoding_LSTM_chunks20.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target shape (825,)\n",
      "Train shape (825, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "h5f = h5py.File(path + filename,'r')\n",
    "X_train = h5f['train'][:]\n",
    "Y_train = h5f['target'][:]\n",
    "h5f.close()\n",
    "\n",
    "print('Target shape', Y_train.shape)\n",
    "print('Train shape', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hold time\n",
    "# X_train = X_train[:,:,1:]\n",
    "\n",
    "# leave only hold time\n",
    "# X_train = X_train[:,:,:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.1, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.    , -0.5   ,  0.1713],\n",
       "       [-2.    ,  0.5   ,  0.1432],\n",
       "       [ 1.    ,  0.5   ,  0.0655],\n",
       "       [-1.    , -0.5   ,  0.1188],\n",
       "       [-3.    ,  0.5   ,  0.0737],\n",
       "       [ 1.    ,  0.5   ,  0.0654],\n",
       "       [-2.    ,  0.5   ,  0.0785],\n",
       "       [-1.    , -0.5   ,  0.2282],\n",
       "       [-3.    ,  0.5   ,  0.0905],\n",
       "       [-2.    , -0.5   ,  0.1666],\n",
       "       [ 1.    , -0.5   ,  0.1218],\n",
       "       [-1.    , -0.5   ,  0.1721],\n",
       "       [-3.    ,  0.5   ,  0.0801],\n",
       "       [ 1.    , -0.5   ,  0.1313],\n",
       "       [ 1.    , -0.5   ,  0.1151],\n",
       "       [ 1.    , -0.5   ,  0.0936],\n",
       "       [-1.    , -0.5   ,  0.1259],\n",
       "       [-3.    ,  0.5   ,  0.0712],\n",
       "       [ 1.    ,  0.5   ,  0.0752],\n",
       "       [ 1.    , -0.5   ,  0.128 ],\n",
       "       [ 1.    ,  0.5   ,  0.0731],\n",
       "       [-2.    ,  0.5   ,  0.0835],\n",
       "       [-2.    , -0.5   ,  0.1317],\n",
       "       [ 1.    , -0.5   ,  0.1147],\n",
       "       [-1.    , -0.5   ,  0.1163],\n",
       "       [-1.    , -0.5   ,  0.1351],\n",
       "       [-1.    , -0.5   ,  0.1014],\n",
       "       [-3.    ,  0.5   ,  0.0759],\n",
       "       [-1.    ,  0.5   ,  0.0649],\n",
       "       [-1.    ,  0.5   ,  0.0734],\n",
       "       [-1.    , -0.5   ,  0.1255],\n",
       "       [-2.    ,  0.5   ,  0.078 ],\n",
       "       [-1.    , -0.5   ,  0.1226],\n",
       "       [-1.    , -0.5   ,  0.1708],\n",
       "       [-1.    , -0.5   ,  0.1313],\n",
       "       [-1.    , -0.5   ,  0.1007],\n",
       "       [-3.    ,  0.5   ,  0.0553],\n",
       "       [-1.    ,  0.5   ,  0.4766],\n",
       "       [-1.    , -0.5   ,  0.1769],\n",
       "       [ 1.    ,  0.5   ,  0.073 ],\n",
       "       [-1.    , -0.5   ,  0.1492],\n",
       "       [-1.    , -0.5   ,  0.1654],\n",
       "       [-2.    ,  0.5   ,  0.0298],\n",
       "       [-3.    ,  0.5   ,  0.0608],\n",
       "       [-1.    ,  0.5   ,  0.085 ],\n",
       "       [ 1.    ,  0.5   ,  0.078 ],\n",
       "       [-1.    , -0.5   ,  0.0918],\n",
       "       [-2.    ,  0.5   ,  0.0919],\n",
       "       [-1.    , -0.5   ,  0.1146],\n",
       "       [-3.    ,  0.5   ,  0.0889],\n",
       "       [ 1.    ,  0.5   ,  0.0776],\n",
       "       [-3.    ,  0.5   ,  0.0698],\n",
       "       [-1.    ,  0.5   ,  0.1835],\n",
       "       [-1.    , -0.5   ,  0.1616],\n",
       "       [-2.    ,  0.5   ,  0.0838],\n",
       "       [-1.    , -0.5   ,  0.1717],\n",
       "       [-3.    ,  0.5   ,  0.1241],\n",
       "       [ 1.    ,  0.5   ,  0.0832],\n",
       "       [ 1.    , -0.5   ,  0.158 ],\n",
       "       [-3.    ,  0.5   ,  0.0738],\n",
       "       [ 1.    , -0.5   ,  0.1493],\n",
       "       [ 1.    , -0.5   ,  0.0861],\n",
       "       [-1.    , -0.5   ,  0.1825],\n",
       "       [-2.    ,  0.5   ,  0.0793],\n",
       "       [-3.    ,  0.5   ,  0.0772],\n",
       "       [-2.    ,  0.5   ,  0.0819],\n",
       "       [ 1.    ,  0.5   ,  0.088 ],\n",
       "       [ 1.    ,  0.5   ,  0.0717],\n",
       "       [-3.    ,  0.5   ,  0.091 ],\n",
       "       [-1.    , -0.5   ,  0.2353],\n",
       "       [-1.    ,  0.5   ,  0.0753],\n",
       "       [ 1.    ,  0.5   ,  0.0342],\n",
       "       [-1.    , -0.5   ,  0.1651],\n",
       "       [-1.    , -0.5   ,  0.1521],\n",
       "       [-2.    ,  0.5   ,  0.0816],\n",
       "       [-3.    ,  0.5   ,  0.0729],\n",
       "       [-1.    , -0.5   ,  0.1389],\n",
       "       [ 1.    ,  0.5   ,  0.0727],\n",
       "       [-1.    , -0.5   ,  0.1686],\n",
       "       [ 3.    , -0.5   ,  0.0553],\n",
       "       [ 1.    ,  0.5   ,  0.0552],\n",
       "       [-1.    ,  0.5   ,  0.0723],\n",
       "       [ 1.    ,  0.5   ,  0.0715],\n",
       "       [-1.    , -0.5   ,  0.156 ],\n",
       "       [-1.    , -0.5   ,  0.1471],\n",
       "       [-1.    , -0.5   ,  0.154 ],\n",
       "       [-1.    , -0.5   ,  0.2006],\n",
       "       [-3.    ,  0.5   ,  0.0828],\n",
       "       [ 1.    ,  0.5   ,  0.0766],\n",
       "       [-3.    ,  0.5   ,  0.0735],\n",
       "       [-1.    , -0.5   ,  0.1257],\n",
       "       [-1.    , -0.5   ,  0.1226],\n",
       "       [-2.    ,  0.5   ,  0.0766],\n",
       "       [-1.    , -0.5   ,  0.1354],\n",
       "       [-3.    ,  0.5   ,  0.0706],\n",
       "       [ 1.    ,  0.5   ,  0.0754],\n",
       "       [-3.    ,  0.5   ,  0.075 ],\n",
       "       [-1.    , -0.5   ,  0.1421],\n",
       "       [ 1.    ,  0.5   ,  0.068 ],\n",
       "       [ 1.    , -0.5   ,  0.1034],\n",
       "       [-2.    ,  0.5   ,  0.0812],\n",
       "       [ 1.    ,  0.5   ,  0.076 ],\n",
       "       [ 1.    , -0.5   ,  0.1163],\n",
       "       [ 1.    , -0.5   ,  0.1991],\n",
       "       [-1.    , -0.5   ,  0.1847],\n",
       "       [-3.    ,  0.5   ,  0.071 ],\n",
       "       [ 1.    , -0.5   ,  0.0989],\n",
       "       [-1.    , -0.5   ,  0.134 ],\n",
       "       [ 1.    , -0.5   ,  0.0728],\n",
       "       [-1.    , -0.5   ,  0.1436],\n",
       "       [-2.    ,  0.5   ,  0.1093],\n",
       "       [-1.    , -0.5   ,  0.0995],\n",
       "       [-2.    ,  0.5   ,  0.0904],\n",
       "       [-3.    ,  0.5   ,  0.0925],\n",
       "       [-2.    ,  0.5   ,  0.0462],\n",
       "       [ 1.    ,  0.5   ,  0.0778],\n",
       "       [ 1.    ,  0.5   ,  0.0745],\n",
       "       [-3.    ,  0.5   ,  0.078 ],\n",
       "       [-1.    , -0.5   ,  0.2147],\n",
       "       [-1.    ,  0.5   ,  0.0653],\n",
       "       [ 1.    , -0.5   ,  0.1714],\n",
       "       [-1.    , -0.5   ,  0.0839],\n",
       "       [ 1.    , -0.5   ,  0.1053],\n",
       "       [ 1.    , -0.5   ,  0.1129],\n",
       "       [-1.    , -0.5   ,  0.1082],\n",
       "       [-3.    ,  0.5   ,  0.1   ],\n",
       "       [ 1.    ,  0.5   ,  0.0846],\n",
       "       [-3.    ,  0.5   ,  0.0794],\n",
       "       [ 1.    ,  0.5   ,  0.0751],\n",
       "       [-2.    ,  0.5   ,  0.0781],\n",
       "       [-3.    ,  0.5   ,  0.0871],\n",
       "       [-1.    , -0.5   ,  0.1438],\n",
       "       [-1.    , -0.5   ,  0.1147],\n",
       "       [-2.    ,  0.5   ,  0.0755],\n",
       "       [-1.    , -0.5   ,  0.1215],\n",
       "       [-1.    , -0.5   ,  0.1164],\n",
       "       [-3.    ,  0.5   ,  0.0832],\n",
       "       [-1.    , -0.5   ,  0.0881],\n",
       "       [ 1.    , -0.5   ,  0.1216],\n",
       "       [-3.    ,  0.5   ,  0.0705],\n",
       "       [-1.    ,  0.5   ,  0.0726],\n",
       "       [ 1.    ,  0.5   ,  0.0657],\n",
       "       [-1.    , -0.5   ,  0.1085],\n",
       "       [-1.    , -0.5   ,  0.1427],\n",
       "       [ 1.    , -0.5   ,  0.0932],\n",
       "       [-3.    ,  0.5   ,  0.0746],\n",
       "       [ 1.    ,  0.5   ,  0.0654],\n",
       "       [-3.    ,  0.5   ,  0.0779],\n",
       "       [-1.    , -0.5   ,  0.1973],\n",
       "       [ 1.    ,  0.5   ,  0.0684],\n",
       "       [-2.    , -0.5   ,  0.1008],\n",
       "       [ 1.    , -0.5   ,  0.1386],\n",
       "       [ 1.    , -0.5   ,  0.1137],\n",
       "       [ 1.    , -0.5   ,  0.0883],\n",
       "       [ 1.    ,  0.5   ,  0.0665],\n",
       "       [ 1.    , -0.5   ,  0.0931],\n",
       "       [-1.    , -0.5   ,  0.1574],\n",
       "       [ 1.    , -0.5   ,  0.0979],\n",
       "       [-2.    ,  0.5   ,  0.0702],\n",
       "       [-1.    ,  0.5   ,  0.0704],\n",
       "       [-2.    ,  0.5   ,  0.0389],\n",
       "       [-1.    ,  0.5   ,  1.2057],\n",
       "       [-1.    , -0.5   ,  0.1816],\n",
       "       [ 1.    ,  0.5   ,  0.0759],\n",
       "       [-1.    , -0.5   ,  0.1285],\n",
       "       [-3.    ,  0.5   ,  0.0732],\n",
       "       [ 1.    ,  0.5   ,  0.0758],\n",
       "       [-1.    , -0.5   ,  0.0866],\n",
       "       [-1.    , -0.5   ,  0.1267],\n",
       "       [ 1.    , -0.5   ,  0.1006],\n",
       "       [ 1.    , -0.5   ,  0.103 ],\n",
       "       [-1.    , -0.5   ,  0.1134],\n",
       "       [-3.    ,  0.5   ,  0.0775],\n",
       "       [-1.    , -0.5   ,  0.0779],\n",
       "       [-1.    ,  0.5   ,  0.0734],\n",
       "       [ 1.    ,  0.5   ,  0.0724],\n",
       "       [-3.    ,  0.5   ,  0.0732],\n",
       "       [-1.    , -0.5   ,  0.1917],\n",
       "       [ 1.    , -0.5   ,  0.0823],\n",
       "       [ 1.    , -0.5   ,  0.1219],\n",
       "       [ 1.    ,  0.5   ,  0.0744],\n",
       "       [ 1.    , -0.5   ,  0.1475],\n",
       "       [-1.    , -0.5   ,  0.1238],\n",
       "       [-3.    ,  0.5   ,  0.0773],\n",
       "       [ 1.    , -0.5   ,  0.1081],\n",
       "       [-1.    , -0.5   ,  0.1937],\n",
       "       [ 1.    , -0.5   ,  0.1454],\n",
       "       [-1.    , -0.5   ,  0.1541],\n",
       "       [-2.    ,  0.5   ,  0.1048],\n",
       "       [-1.    , -0.5   ,  0.1964],\n",
       "       [-2.    ,  0.5   ,  0.0827],\n",
       "       [-3.    ,  0.5   ,  0.0771],\n",
       "       [-2.    ,  0.5   ,  0.0927],\n",
       "       [ 1.    ,  0.5   ,  0.0812],\n",
       "       [ 1.    ,  0.5   ,  0.0702],\n",
       "       [-3.    ,  0.5   ,  0.0824],\n",
       "       [-2.    , -0.5   ,  0.1766],\n",
       "       [ 1.    ,  0.5   ,  0.0725],\n",
       "       [-2.    ,  0.5   ,  0.0769],\n",
       "       [ 1.    , -0.5   ,  0.0917]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 100)               21600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 23,233\n",
      "Trainable params: 23,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "Epoch 1/10\n",
      "742/742 [==============================] - 26s 35ms/step - loss: 0.6940 - binary_accuracy: 0.5081\n",
      "Epoch 2/10\n",
      "742/742 [==============================] - 19s 26ms/step - loss: 0.6908 - binary_accuracy: 0.5377\n",
      "Epoch 3/10\n",
      "742/742 [==============================] - 19s 26ms/step - loss: 0.6821 - binary_accuracy: 0.5741\n",
      "Epoch 4/10\n",
      "742/742 [==============================] - 19s 26ms/step - loss: 0.7201 - binary_accuracy: 0.5243\n",
      "Epoch 5/10\n",
      "742/742 [==============================] - 19s 26ms/step - loss: 0.6838 - binary_accuracy: 0.5660\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(14)  # fix the random numbers generator state\n",
    "\n",
    "batch_size = 16\n",
    "hidden_units = 50\n",
    "input_shape = X_train.shape[1:]\n",
    "nb_epochs = 10\n",
    "nb_classes = 1\n",
    "dropout = 0.05\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0.01, patience=2, verbose=1)\n",
    "print('Build model...')\n",
    "\n",
    "# sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(units=hidden_units, kernel_initializer='uniform', recurrent_initializer='uniform', \n",
    "               dropout=dropout, use_bias=True, unit_forget_bias=True, activation='tanh', recurrent_activation='sigmoid', \n",
    "               input_shape=input_shape), input_shape=input_shape, merge_mode='concat'))\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', metrics=['binary_accuracy'], optimizer='adam')\n",
    "\n",
    "print(\"Train...\")\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epochs, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "# Y_pred = model.predict(X_test)\n",
    "metr = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC 0.6083333333333334\n",
      "Accuracy 0.5421686746987951\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict_proba(X_test)\n",
    "print('AUC', roc_auc_score(Y_test, Y_pred))\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "print('Accuracy', accuracy_score(Y_test, np.round(Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4778933 , 0.4870424 , 0.52825695, 0.53514063, 0.5212515 ,\n",
       "        0.5018703 , 0.4804341 , 0.4918298 , 0.4461914 , 0.4840215 ,\n",
       "        0.4892945 , 0.46968076, 0.49022916, 0.48221594, 0.489561  ,\n",
       "        0.51989245, 0.5205368 , 0.49398917, 0.46904454, 0.5228303 ,\n",
       "        0.51616645, 0.50427663, 0.48162362, 0.49703497, 0.50298166,\n",
       "        0.5224433 , 0.47580087, 0.5227336 , 0.4873378 , 0.5169181 ,\n",
       "        0.48274875, 0.5098665 , 0.4912343 , 0.49727383, 0.5066752 ,\n",
       "        0.48415858, 0.4842803 , 0.51625085, 0.47838178, 0.5040419 ,\n",
       "        0.47343567, 0.47444442, 0.4846944 , 0.52918994, 0.5175565 ,\n",
       "        0.51232195, 0.49892247, 0.48431656, 0.5163459 , 0.49553385,\n",
       "        0.49651223, 0.5019428 , 0.4834364 , 0.5245507 , 0.5266129 ,\n",
       "        0.53218454, 0.527812  , 0.5036077 , 0.5094161 , 0.5259893 ,\n",
       "        0.45883435, 0.5132012 , 0.522447  , 0.46684578, 0.5311623 ,\n",
       "        0.50011975, 0.49221826, 0.46272397, 0.52467567, 0.4841633 ,\n",
       "        0.49146664, 0.49775565, 0.4788566 , 0.50072664, 0.462972  ,\n",
       "        0.5220108 , 0.4951733 , 0.49557975, 0.4807081 , 0.5058453 ,\n",
       "        0.51550853, 0.5029763 , 0.48538455]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = model.predict_proba(X_test)\n",
    "Y_pred.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]),\n",
       " array([[0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
       "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "         1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "         1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
       "         1., 1., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test, np.round(Y_pred).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train indices: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  35  36  37\n",
      "  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  54  55  56\n",
      "  57  58  59  60  62  64  65  66  67  68  69  70  71  72  73  74  75  76\n",
      "  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  95\n",
      "  96  97  99 100 101 102 103 104 105 106 107 109 110 111 112 113 114 115\n",
      " 116 117 118 119 120 121 122 123 124 126 127 128 129 130 131 132 133 134\n",
      " 135 136 137 139 141 143 144 145 146 147 148 149 150 151 152 153 154 155\n",
      " 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173\n",
      " 174 175 176 177 178 179 180 181 182 183 184 185 187 189 190 191 192 193\n",
      " 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 210 211 212\n",
      " 213 214 215 216 217 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233 234 235 236 237 238 240 241 242 243 244 245 246 247 248 249 250\n",
      " 251 253 254 255 257 258 259 260 261 262 264 265 266 267 268 269 271 274\n",
      " 275 276 277 278 279 280 281 282 283 284 285 286 287 288 290 291 292 293\n",
      " 294 295 296 297 298 299 301 302 303 304 305 307 308 309 310 311 312 313\n",
      " 314 315 316 317 319 320 321 322 323 324 325 328 329 330 331 332 333 334\n",
      " 335 337 338 339 340 341 342 343 344 345 346 347 348 349 351 353 354 355\n",
      " 356 357 358 359 360 361 362 363 364 365 366 368 369 370 371 372 373 374\n",
      " 375 376 377 378 379 380 381 383 384 385 386 390 391 393 394 395 397 398\n",
      " 400 401 402 403 404 405 406 407 408 410 411 412 413 414 415 416 419 420\n",
      " 421 422 423 424 425 426 427 428 429 430 432 433 434 435 436 437 438 439\n",
      " 440 441 442 443 444 445 446 447 448 449 450 451 452 454 455 456 458 460\n",
      " 461 462 463 464 466 468 469 470 471 473 474 475 476 477 479 481 483 484\n",
      " 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502\n",
      " 503 504 505 506 507 508 509 510 511 512 513 516 517 518 519 520 521 522\n",
      " 523 524 525 526 527 529 531 533 534 535 536 537 538 539 540 541 542 543\n",
      " 544 545 546 547 548 549 550 551 553 554 555 556 557 558 559 560 561 563\n",
      " 564 565 566 567 569 570 571 572 573 574 575 576 577 578 579 580 581 582\n",
      " 583 584 585 586 587 588 589 590 591 592 593 594 596 597 598 599 600 601\n",
      " 603 604 605 606 608 609 610 611 612 613 614 615 617 618 619 620 621 622\n",
      " 624 625 626 627 628 629 630 631 632 633 634 636 637 638 639 641 642 644\n",
      " 646 647 648 649 650 651 652 653 654 655 656 657 658 659 661 663 664 665\n",
      " 666 667 668 669 670 671 672 674 675 676 677 678 679 680 681 682 683 684\n",
      " 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702\n",
      " 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720\n",
      " 721 722 723 724 726 728 729 730 731 732 733 734 735 736 737 738 739 740\n",
      " 741 742 743 744 745 748 749 750 751 753 754 755 756 757 758 759 760 761\n",
      " 762 763 764 765 767 768 769 770 771 772 773 774 775 776 777 778 779 780\n",
      " 781 782 784 785 786 787 789 790 791 792 793 794 795 796 797 798 799 800\n",
      " 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819\n",
      " 820 821 822 824]\n",
      "Test indices: [ 13  34  53  61  63  94  98 108 125 138 140 142 186 188 209 218 239 252\n",
      " 256 263 270 272 273 289 300 306 318 326 327 336 350 352 367 382 387 388\n",
      " 389 392 396 399 409 417 418 431 453 457 459 465 467 472 478 480 482 514\n",
      " 515 528 530 532 552 562 568 595 602 607 616 623 635 640 643 645 660 662\n",
      " 673 725 727 746 747 752 766 783 788 801 823]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 50)                10800     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,633\n",
      "Trainable params: 11,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "Train on 630 samples, validate on 112 samples\n",
      "Epoch 1/20\n",
      "630/630 [==============================] - 12s 20ms/step - loss: 0.6943 - binary_accuracy: 0.5048 - val_loss: 0.6908 - val_binary_accuracy: 0.4911\n",
      "Epoch 2/20\n",
      "630/630 [==============================] - 10s 16ms/step - loss: 0.6917 - binary_accuracy: 0.5317 - val_loss: 0.7029 - val_binary_accuracy: 0.3661\n",
      "Epoch 3/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6911 - binary_accuracy: 0.5127 - val_loss: 0.7022 - val_binary_accuracy: 0.4018\n",
      "Epoch 4/20\n",
      "630/630 [==============================] - 10s 16ms/step - loss: 0.6852 - binary_accuracy: 0.5444 - val_loss: 0.7670 - val_binary_accuracy: 0.3393\n",
      "Epoch 00004: early stopping\n",
      "Accuracy is 0.5542168674698795\n",
      "AUC is 0.7604895104895104\n",
      "Train indices: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  16  17  18  19\n",
      "  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
      "  38  39  40  41  42  43  44  45  47  49  51  52  53  55  56  58  59  61\n",
      "  62  63  65  67  68  69  70  71  72  73  74  75  76  77  78  79  80  82\n",
      "  83  84  85  86  87  88  89  90  91  92  93  94  95  96  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 111 112 113 114 115 116 117 118 119 120\n",
      " 121 122 123 124 125 126 128 129 130 131 132 133 135 136 137 138 139 140\n",
      " 141 142 143 144 145 147 149 150 151 152 154 155 156 157 158 160 161 162\n",
      " 163 164 165 166 167 169 170 171 172 173 174 175 176 177 178 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200\n",
      " 201 202 203 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\n",
      " 221 222 223 224 225 226 227 229 230 231 232 233 234 235 236 237 238 239\n",
      " 240 241 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258\n",
      " 260 261 262 263 265 266 267 268 269 270 272 273 274 275 276 279 280 281\n",
      " 282 283 284 285 286 287 288 289 290 291 292 293 295 296 297 298 299 300\n",
      " 301 303 304 306 307 308 309 310 311 312 313 314 315 317 318 319 320 321\n",
      " 322 323 324 325 326 327 328 329 330 331 332 333 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349 350 351 352 353 354 356 357 358 359\n",
      " 360 361 363 364 365 366 367 368 369 370 371 373 374 375 376 377 378 379\n",
      " 380 381 382 383 384 385 386 387 388 389 391 392 393 394 395 396 397 398\n",
      " 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416\n",
      " 417 418 419 420 421 422 423 424 425 427 428 429 430 431 432 433 434 435\n",
      " 436 437 438 439 440 442 443 444 445 446 447 448 449 450 451 452 453 456\n",
      " 457 458 459 460 461 462 464 465 467 468 469 470 471 472 473 474 475 476\n",
      " 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494\n",
      " 495 496 497 498 499 501 502 503 504 505 506 507 508 510 511 512 513 514\n",
      " 515 516 517 518 519 520 521 522 524 525 527 528 529 530 532 533 534 535\n",
      " 536 537 539 541 542 544 545 546 547 548 549 550 551 552 553 554 555 556\n",
      " 557 558 559 560 561 562 563 564 566 568 569 571 572 573 574 575 576 577\n",
      " 578 579 580 581 582 583 584 586 587 588 589 591 592 594 595 596 599 600\n",
      " 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618\n",
      " 619 620 621 622 623 624 625 627 628 629 631 633 634 635 636 637 638 639\n",
      " 640 641 642 643 644 645 646 647 648 649 650 652 653 654 656 657 658 659\n",
      " 660 661 662 663 664 666 667 668 669 670 671 672 673 674 675 676 677 678\n",
      " 680 681 682 684 686 687 688 689 690 691 692 693 694 695 696 697 698 699\n",
      " 700 701 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 735 737 738 740\n",
      " 741 743 744 745 746 747 748 749 750 752 753 754 756 757 758 759 760 761\n",
      " 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780\n",
      " 781 782 783 784 785 786 787 788 790 791 792 793 794 795 796 797 798 799\n",
      " 800 801 802 803 804 805 806 807 809 810 811 812 814 815 816 817 818 819\n",
      " 820 821 822 823]\n",
      "Test indices: [  9  15  46  48  50  54  57  60  64  66  81  97 110 127 134 146 148 153\n",
      " 159 168 179 204 220 228 242 259 264 271 277 278 294 302 305 316 334 355\n",
      " 362 372 390 426 441 454 455 463 466 500 509 523 526 531 538 540 543 565\n",
      " 567 570 585 590 593 597 598 626 630 632 651 655 665 679 683 685 702 719\n",
      " 734 736 739 742 751 755 762 789 808 813 824]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 50)                10800     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,633\n",
      "Trainable params: 11,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 630 samples, validate on 112 samples\n",
      "Epoch 1/20\n",
      "630/630 [==============================] - 12s 20ms/step - loss: 0.6948 - binary_accuracy: 0.5000 - val_loss: 0.7170 - val_binary_accuracy: 0.3304\n",
      "Epoch 2/20\n",
      "630/630 [==============================] - 10s 16ms/step - loss: 0.6898 - binary_accuracy: 0.5190 - val_loss: 0.6973 - val_binary_accuracy: 0.3571\n",
      "Epoch 3/20\n",
      "630/630 [==============================] - 10s 16ms/step - loss: 0.6808 - binary_accuracy: 0.5571 - val_loss: 0.6828 - val_binary_accuracy: 0.5804\n",
      "Epoch 4/20\n",
      "630/630 [==============================] - 10s 16ms/step - loss: 0.6820 - binary_accuracy: 0.5714 - val_loss: 0.6924 - val_binary_accuracy: 0.4821\n",
      "Epoch 5/20\n",
      "630/630 [==============================] - 10s 16ms/step - loss: 0.6706 - binary_accuracy: 0.5540 - val_loss: 0.7108 - val_binary_accuracy: 0.4464\n",
      "Epoch 6/20\n",
      "630/630 [==============================] - 10s 16ms/step - loss: 0.9375 - binary_accuracy: 0.5508 - val_loss: 0.6618 - val_binary_accuracy: 0.6696\n",
      "Epoch 7/20\n",
      "630/630 [==============================] - 10s 16ms/step - loss: 0.6897 - binary_accuracy: 0.5143 - val_loss: 0.6753 - val_binary_accuracy: 0.6786\n",
      "Epoch 8/20\n",
      "630/630 [==============================] - 10s 16ms/step - loss: 0.6854 - binary_accuracy: 0.5778 - val_loss: 0.6918 - val_binary_accuracy: 0.4821\n",
      "Epoch 9/20\n",
      "630/630 [==============================] - 10s 16ms/step - loss: 0.6842 - binary_accuracy: 0.5286 - val_loss: 0.6973 - val_binary_accuracy: 0.4196\n",
      "Epoch 00009: early stopping\n",
      "Accuracy is 0.40963855421686746\n",
      "AUC is 0.5900360144057623\n",
      "Train indices: [  0   1   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  23  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
      "  40  41  42  43  44  45  46  47  48  49  50  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  66  67  68  69  71  72  73  75  76  77  78  79\n",
      "  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97\n",
      "  98 100 101 102 103 104 105 106 107 108 110 111 112 113 114 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 129 130 131 133 134 135 136 137 138\n",
      " 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156\n",
      " 157 158 159 160 161 162 163 164 165 166 167 168 169 170 172 173 174 175\n",
      " 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 193 194 195\n",
      " 196 197 198 200 201 202 203 204 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 223 224 225 226 227 228 230 231 233 234 235 236\n",
      " 237 238 239 240 241 242 243 245 246 247 248 249 250 252 254 255 256 257\n",
      " 258 259 260 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276\n",
      " 277 278 279 280 281 282 284 285 286 287 288 289 290 291 292 293 294 295\n",
      " 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314\n",
      " 315 316 317 318 319 320 321 322 324 325 326 327 328 329 330 331 332 333\n",
      " 334 335 336 337 338 339 340 341 342 343 344 345 346 348 349 350 351 352\n",
      " 354 355 356 358 359 361 362 363 364 366 367 368 369 370 372 373 374 376\n",
      " 377 378 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 404 405 406 408 409 410 412 413 415 416 417\n",
      " 418 419 420 421 422 424 425 426 427 428 429 430 431 432 433 434 435 436\n",
      " 437 438 439 440 441 443 444 445 446 447 448 449 450 452 453 454 455 456\n",
      " 457 458 459 460 461 462 463 464 465 466 467 468 469 470 472 473 474 475\n",
      " 476 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494\n",
      " 495 496 497 499 500 501 502 503 504 505 506 507 509 511 512 513 514 515\n",
      " 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533\n",
      " 534 535 536 537 538 539 540 541 543 544 545 546 548 549 550 552 553 554\n",
      " 555 556 558 559 560 561 562 565 566 567 568 569 570 573 574 575 576 577\n",
      " 578 579 580 581 583 584 585 586 588 589 590 591 593 594 595 596 597 598\n",
      " 599 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618\n",
      " 619 620 621 622 623 624 626 627 628 629 630 631 632 633 634 635 636 637\n",
      " 639 640 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657\n",
      " 658 659 660 661 662 664 665 666 667 669 670 671 672 673 674 675 676 678\n",
      " 679 680 681 682 683 685 686 687 688 689 690 691 693 694 695 696 697 698\n",
      " 699 700 701 702 703 704 706 707 708 709 710 711 712 713 714 715 716 717\n",
      " 718 719 720 721 723 724 725 726 727 728 729 730 731 732 733 734 735 736\n",
      " 737 738 739 740 741 742 743 744 745 746 747 748 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 766 767 768 770 772 773 774 775 776\n",
      " 777 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795\n",
      " 797 798 799 800 801 802 804 805 806 807 808 809 810 811 813 815 817 818\n",
      " 819 820 823 824]\n",
      "Test indices: [  2  22  24  25  51  65  70  74  99 109 115 128 132 171 191 192 199 205\n",
      " 222 229 232 244 251 253 261 283 296 323 347 353 357 360 365 371 375 379\n",
      " 403 407 411 414 423 442 451 471 477 498 508 510 542 547 551 557 563 564\n",
      " 571 572 582 587 592 600 601 625 638 641 663 668 677 684 692 705 722 749\n",
      " 765 769 771 778 796 803 812 814 816 821 822]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 50)                10800     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,633\n",
      "Trainable params: 11,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "Train on 630 samples, validate on 112 samples\n",
      "Epoch 1/20\n",
      "630/630 [==============================] - 12s 20ms/step - loss: 0.6942 - binary_accuracy: 0.5159 - val_loss: 0.7033 - val_binary_accuracy: 0.2946\n",
      "Epoch 2/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6931 - binary_accuracy: 0.5032 - val_loss: 0.7026 - val_binary_accuracy: 0.2946\n",
      "Epoch 3/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6906 - binary_accuracy: 0.5190 - val_loss: 0.7052 - val_binary_accuracy: 0.3214\n",
      "Epoch 4/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6890 - binary_accuracy: 0.5222 - val_loss: 0.6597 - val_binary_accuracy: 0.6964\n",
      "Epoch 5/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6899 - binary_accuracy: 0.5429 - val_loss: 0.6767 - val_binary_accuracy: 0.6875\n",
      "Epoch 6/20\n",
      "630/630 [==============================] - 10s 16ms/step - loss: 0.6889 - binary_accuracy: 0.5746 - val_loss: 0.6898 - val_binary_accuracy: 0.5268\n",
      "Epoch 7/20\n",
      "630/630 [==============================] - 10s 16ms/step - loss: 0.6851 - binary_accuracy: 0.5571 - val_loss: 0.6898 - val_binary_accuracy: 0.5179\n",
      "Epoch 00007: early stopping\n",
      "Accuracy is 0.6626506024096386\n",
      "AUC is 0.7279761904761904\n",
      "Train indices: [  0   1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  18  19\n",
      "  20  21  22  24  25  26  28  29  31  32  33  34  35  36  38  39  40  41\n",
      "  42  43  44  45  46  47  48  49  50  51  53  54  56  57  59  60  61  62\n",
      "  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80\n",
      "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  97  98  99\n",
      " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 115 118 119 121\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
      " 140 142 143 144 145 146 147 148 150 151 152 153 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 194 195 196 197 198 199\n",
      " 201 203 204 205 206 208 209 210 211 212 213 214 215 216 218 219 220 222\n",
      " 224 225 226 227 228 229 230 231 232 233 234 237 238 239 240 241 242 243\n",
      " 244 245 246 247 250 251 252 253 254 255 256 257 258 259 260 261 262 263\n",
      " 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 281 282\n",
      " 283 284 286 287 288 289 290 291 292 293 294 295 296 297 298 300 301 302\n",
      " 303 304 305 306 309 310 311 312 313 314 315 316 317 318 319 320 321 322\n",
      " 323 324 325 326 327 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 352 353 354 355 356 357 358 359 360\n",
      " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
      " 379 380 382 383 384 386 387 388 389 390 391 392 393 394 395 396 397 398\n",
      " 399 400 401 402 403 404 405 406 407 409 410 411 412 413 414 415 416 417\n",
      " 418 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 438\n",
      " 439 440 441 442 443 444 446 447 448 449 450 451 452 453 454 455 456 457\n",
      " 458 459 460 461 462 463 465 466 467 468 469 470 471 472 473 474 475 476\n",
      " 477 478 479 480 481 482 483 485 486 487 488 489 490 491 492 493 494 495\n",
      " 496 497 498 499 500 501 502 504 505 506 507 508 509 510 512 513 514 515\n",
      " 516 517 518 519 520 521 522 523 524 526 527 528 529 530 531 532 533 534\n",
      " 535 536 537 538 539 540 541 542 543 545 546 547 548 549 550 551 552 554\n",
      " 555 556 557 558 559 560 561 562 563 564 565 566 567 568 570 571 572 573\n",
      " 574 575 576 577 578 579 580 581 582 583 584 585 587 588 589 590 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 609 610 611 612\n",
      " 613 614 615 616 617 619 622 623 624 625 626 627 628 629 630 631 632 633\n",
      " 634 635 636 637 638 639 640 641 643 644 645 646 647 648 649 651 652 654\n",
      " 655 657 658 659 660 661 662 663 664 665 667 668 670 671 672 673 675 676\n",
      " 677 678 679 680 681 682 683 684 685 687 688 690 691 692 693 695 696 697\n",
      " 699 700 701 702 703 704 705 707 709 710 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 732 733 734 735 736 737 738\n",
      " 739 741 742 743 744 745 746 747 748 749 751 752 753 754 755 756 758 759\n",
      " 761 762 763 764 765 766 768 769 770 771 772 773 774 775 777 778 780 781\n",
      " 782 783 784 785 786 787 788 789 790 792 793 794 795 796 797 798 799 801\n",
      " 802 803 804 805 806 807 808 809 811 812 813 814 815 816 817 818 819 820\n",
      " 821 822 823 824]\n",
      "Test indices: [  3  17  23  27  30  37  52  55  58  96 101 116 117 120 141 149 154 155\n",
      " 177 193 200 202 207 217 221 223 235 236 248 249 280 285 299 307 308 328\n",
      " 351 381 385 408 419 420 437 445 464 484 503 511 525 544 553 569 586 591\n",
      " 608 618 620 621 642 650 653 656 666 669 674 686 689 694 698 706 708 711\n",
      " 731 740 750 757 760 767 776 779 791 800 810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 50)                10800     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,633\n",
      "Trainable params: 11,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "Train on 630 samples, validate on 112 samples\n",
      "Epoch 1/20\n",
      "630/630 [==============================] - 12s 20ms/step - loss: 0.6952 - binary_accuracy: 0.4810 - val_loss: 0.7074 - val_binary_accuracy: 0.3304\n",
      "Epoch 2/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6915 - binary_accuracy: 0.5143 - val_loss: 0.7026 - val_binary_accuracy: 0.3393\n",
      "Epoch 3/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6896 - binary_accuracy: 0.5222 - val_loss: 0.6951 - val_binary_accuracy: 0.4286\n",
      "Epoch 4/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6899 - binary_accuracy: 0.5206 - val_loss: 0.6749 - val_binary_accuracy: 0.7321\n",
      "Epoch 5/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6855 - binary_accuracy: 0.5492 - val_loss: 0.6967 - val_binary_accuracy: 0.4554\n",
      "Epoch 6/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6858 - binary_accuracy: 0.5460 - val_loss: 0.6494 - val_binary_accuracy: 0.6696\n",
      "Epoch 7/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6908 - binary_accuracy: 0.5349 - val_loss: 0.6752 - val_binary_accuracy: 0.6964\n",
      "Epoch 8/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6861 - binary_accuracy: 0.6175 - val_loss: 0.6802 - val_binary_accuracy: 0.6339\n",
      "Epoch 9/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6842 - binary_accuracy: 0.5619 - val_loss: 0.6885 - val_binary_accuracy: 0.4911\n",
      "Epoch 00009: early stopping\n",
      "Accuracy is 0.5301204819277109\n",
      "AUC is 0.5932400932400932\n",
      "Train indices: [  0   1   2   3   4   5   6   8   9  11  12  13  14  15  16  17  18  19\n",
      "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  35  36  37  39\n",
      "  41  42  44  45  46  47  48  49  50  51  52  53  54  55  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  72  73  74  75  76  77  78  79\n",
      "  80  81  83  84  85  86  88  91  92  94  95  96  97  98  99 100 101 102\n",
      " 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 120 121 123\n",
      " 124 125 126 127 128 129 130 131 132 134 135 136 137 138 139 140 141 142\n",
      " 143 144 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 164 165 167 168 169 171 174 175 176 177 178 179 180 181 182 183 184\n",
      " 185 186 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203\n",
      " 204 205 206 207 208 209 210 211 212 215 216 217 218 219 220 221 222 223\n",
      " 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 241 242\n",
      " 244 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 263\n",
      " 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 282\n",
      " 283 285 286 287 288 289 290 292 293 294 295 296 297 298 299 300 301 302\n",
      " 303 305 306 307 308 309 311 312 313 314 315 316 317 318 320 322 323 324\n",
      " 325 326 327 328 329 330 331 332 334 335 336 337 338 339 340 341 342 343\n",
      " 345 347 348 349 350 351 352 353 354 355 356 357 360 361 362 363 364 365\n",
      " 366 367 368 369 370 371 372 373 374 375 377 378 379 380 381 382 383 385\n",
      " 386 387 388 389 390 391 392 393 394 396 398 399 400 401 402 403 404 405\n",
      " 406 407 408 409 410 411 413 414 415 416 417 418 419 420 421 423 424 425\n",
      " 426 427 428 429 431 432 433 434 436 437 438 439 440 441 442 443 444 445\n",
      " 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463\n",
      " 464 465 466 467 468 469 471 472 474 475 476 477 478 479 480 481 482 483\n",
      " 484 486 488 489 490 493 494 495 496 497 498 500 501 502 503 504 505 506\n",
      " 508 509 510 511 512 514 515 516 517 519 520 521 522 523 524 525 526 527\n",
      " 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545\n",
      " 546 547 548 549 550 551 552 553 554 555 556 557 559 560 561 562 563 564\n",
      " 565 566 567 568 569 570 571 572 573 575 576 577 578 579 580 582 583 584\n",
      " 585 586 587 588 589 590 591 592 593 595 597 598 599 600 601 602 603 604\n",
      " 605 606 607 608 609 610 611 612 614 615 616 617 618 619 620 621 622 623\n",
      " 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 640 641 642\n",
      " 643 645 646 647 648 649 650 651 652 653 654 655 656 658 659 660 661 662\n",
      " 663 664 665 666 667 668 669 670 671 672 673 674 675 677 678 679 680 681\n",
      " 682 683 684 685 686 687 689 690 691 692 693 694 695 696 697 698 699 700\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 716 717 718 719 721\n",
      " 722 723 725 726 727 728 729 730 731 732 733 734 735 736 738 739 740 741\n",
      " 742 745 746 747 749 750 751 752 753 754 755 756 757 758 759 760 761 762\n",
      " 763 764 765 766 767 768 769 771 772 773 774 775 776 777 778 779 780 781\n",
      " 783 784 785 787 788 789 790 791 792 793 794 795 796 797 798 800 801 802\n",
      " 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820\n",
      " 821 822 823 824]\n",
      "Test indices: [  7  10  32  38  40  43  56  71  82  87  89  90  93 118 119 122 133 145\n",
      " 163 166 170 172 173 187 213 214 240 243 245 262 281 284 291 304 310 319\n",
      " 321 333 344 346 358 359 376 384 395 397 412 422 430 435 470 473 485 487\n",
      " 491 492 499 507 513 518 558 574 581 594 596 613 639 644 657 676 688 701\n",
      " 715 720 724 737 743 744 748 770 782 786 799]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 50)                10800     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,633\n",
      "Trainable params: 11,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "Train on 630 samples, validate on 112 samples\n",
      "Epoch 1/20\n",
      "630/630 [==============================] - 12s 20ms/step - loss: 0.6957 - binary_accuracy: 0.4825 - val_loss: 0.6897 - val_binary_accuracy: 0.5714\n",
      "Epoch 2/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6927 - binary_accuracy: 0.5095 - val_loss: 0.7059 - val_binary_accuracy: 0.3214\n",
      "Epoch 3/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6901 - binary_accuracy: 0.5079 - val_loss: 0.6785 - val_binary_accuracy: 0.6518\n",
      "Epoch 4/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6922 - binary_accuracy: 0.5270 - val_loss: 0.6663 - val_binary_accuracy: 0.6875\n",
      "Epoch 5/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6908 - binary_accuracy: 0.5429 - val_loss: 0.6799 - val_binary_accuracy: 0.6161\n",
      "Epoch 6/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6879 - binary_accuracy: 0.5714 - val_loss: 0.6941 - val_binary_accuracy: 0.5357\n",
      "Epoch 7/20\n",
      "630/630 [==============================] - 10s 15ms/step - loss: 0.6887 - binary_accuracy: 0.5333 - val_loss: 0.6842 - val_binary_accuracy: 0.5804\n",
      "Epoch 00007: early stopping\n",
      "Accuracy is 0.5903614457831325\n",
      "AUC is 0.659698025551684\n",
      "Train indices: [  1   2   3   5   6   7   8   9  10  11  12  13  15  16  17  18  19  21\n",
      "  22  23  24  25  27  29  30  32  33  34  35  36  37  38  39  40  42  43\n",
      "  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  60  61  62\n",
      "  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  81  82\n",
      "  83  84  85  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 115 116 117 118 119 120 121\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 134 136 137 138 139 140\n",
      " 141 142 143 144 145 146 148 149 151 152 153 154 155 156 157 159 160 161\n",
      " 163 166 167 168 169 170 171 172 173 175 176 177 178 179 180 181 182 183\n",
      " 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201\n",
      " 202 203 204 205 206 207 208 209 210 211 212 213 214 215 217 218 219 220\n",
      " 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n",
      " 257 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275\n",
      " 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 294 295\n",
      " 296 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 315\n",
      " 316 317 318 319 321 323 325 326 327 328 331 332 333 334 336 337 338 340\n",
      " 341 342 344 345 346 347 348 349 350 351 352 353 355 356 357 358 359 360\n",
      " 362 363 365 366 367 369 371 372 373 374 375 376 378 379 380 381 382 383\n",
      " 384 385 386 387 388 389 390 391 392 393 395 396 397 398 399 400 402 403\n",
      " 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422\n",
      " 423 426 427 428 430 431 432 433 434 435 436 437 438 439 440 441 442 443\n",
      " 444 445 448 450 451 453 454 455 456 457 458 459 460 461 462 463 464 465\n",
      " 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483\n",
      " 484 485 486 487 488 490 491 492 494 495 496 497 498 499 500 502 503 506\n",
      " 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524\n",
      " 525 526 527 528 529 530 531 532 533 535 536 537 538 539 540 542 543 544\n",
      " 546 547 548 550 551 552 553 554 555 557 558 559 560 561 562 563 564 565\n",
      " 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583\n",
      " 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601\n",
      " 602 604 605 607 608 611 612 613 615 616 617 618 619 620 621 622 623 625\n",
      " 626 627 628 629 630 631 632 633 634 635 636 638 639 640 641 642 643 644\n",
      " 645 646 647 648 649 650 651 653 654 655 656 657 658 659 660 661 662 663\n",
      " 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681\n",
      " 682 683 684 685 686 688 689 690 691 692 694 696 697 698 699 701 702 703\n",
      " 704 705 706 708 710 711 712 713 714 715 716 718 719 720 722 723 724 725\n",
      " 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743\n",
      " 744 745 746 747 748 749 750 751 752 753 755 756 757 758 759 760 761 762\n",
      " 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 781\n",
      " 782 783 784 785 786 787 788 789 790 791 792 794 795 796 797 798 799 800\n",
      " 801 802 803 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819\n",
      " 820 821 822 823 824]\n",
      "Test indices: [  0   4  14  20  26  28  31  41  59  79  80  86 113 114 135 147 150 158\n",
      " 162 164 165 174 216 258 292 293 297 314 320 322 324 329 330 335 339 343\n",
      " 354 361 364 368 370 377 394 401 404 424 425 429 446 447 449 452 489 493\n",
      " 501 504 505 534 541 545 549 556 603 606 609 610 614 624 637 652 687 693\n",
      " 695 700 707 709 717 721 754 780 793 804]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_17 (LSTM)               (None, 50)                10800     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,633\n",
      "Trainable params: 11,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 631 samples, validate on 112 samples\n",
      "Epoch 1/20\n",
      "631/631 [==============================] - 12s 20ms/step - loss: 0.6947 - binary_accuracy: 0.4960 - val_loss: 0.6985 - val_binary_accuracy: 0.3482\n",
      "Epoch 2/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5151 - val_loss: 0.7016 - val_binary_accuracy: 0.3750\n",
      "Epoch 3/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6898 - binary_accuracy: 0.5420 - val_loss: 0.7247 - val_binary_accuracy: 0.3661\n",
      "Epoch 4/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6774 - binary_accuracy: 0.5721 - val_loss: 0.8246 - val_binary_accuracy: 0.4196\n",
      "Epoch 00004: early stopping\n",
      "Accuracy is 0.4146341463414634\n",
      "AUC is 0.7179962894248608\n",
      "Train indices: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  19\n",
      "  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  37  38\n",
      "  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56\n",
      "  57  58  59  60  61  63  64  65  66  67  68  69  70  71  72  74  75  76\n",
      "  77  78  79  80  81  82  83  85  86  87  89  90  92  93  94  95  96  97\n",
      "  98  99 100 101 105 106 107 108 109 110 112 113 114 115 116 117 118 119\n",
      " 120 122 123 125 127 128 130 131 132 133 134 135 137 138 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 179 180 182\n",
      " 183 185 186 187 188 189 190 191 192 193 195 196 197 199 200 201 202 203\n",
      " 204 205 207 208 209 210 211 213 214 215 216 217 218 219 220 221 222 223\n",
      " 224 225 228 229 230 231 232 233 234 235 236 239 240 242 243 244 245 247\n",
      " 248 249 250 251 252 253 255 256 257 258 259 260 261 262 263 264 265 266\n",
      " 269 270 271 272 273 276 277 278 279 280 281 282 283 284 285 286 287 288\n",
      " 289 290 291 292 293 294 296 297 298 299 300 302 304 305 306 307 308 309\n",
      " 310 312 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329\n",
      " 330 331 332 333 334 335 336 338 339 341 343 344 345 346 347 348 349 350\n",
      " 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368\n",
      " 369 370 371 372 373 374 375 376 377 379 380 381 382 384 385 387 388 389\n",
      " 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 407 408 409\n",
      " 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427\n",
      " 428 429 430 431 432 434 435 436 437 438 440 441 442 443 444 445 446 447\n",
      " 448 449 451 452 453 454 455 456 457 459 460 461 462 463 464 465 466 467\n",
      " 469 470 471 472 473 474 475 476 477 478 480 481 482 483 484 485 486 487\n",
      " 488 489 490 491 492 493 494 495 497 498 499 500 501 502 503 504 505 506\n",
      " 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524\n",
      " 525 526 528 529 530 531 532 534 535 536 537 538 539 540 541 542 543 544\n",
      " 545 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563\n",
      " 564 565 566 567 568 569 570 571 572 573 574 575 576 579 581 582 583 584\n",
      " 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602\n",
      " 603 604 605 606 607 608 609 610 612 613 614 615 616 617 618 620 621 622\n",
      " 623 624 625 626 628 629 630 631 632 633 634 635 636 637 638 639 640 641\n",
      " 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659\n",
      " 660 661 662 663 664 665 666 667 668 669 670 671 673 674 675 676 677 678\n",
      " 679 681 683 684 685 686 687 688 689 690 692 693 694 695 696 697 698 700\n",
      " 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 719\n",
      " 720 721 722 724 725 726 727 729 730 731 733 734 735 736 737 738 739 740\n",
      " 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758\n",
      " 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776\n",
      " 777 778 779 780 782 783 786 787 788 789 790 791 792 793 794 796 798 799\n",
      " 800 801 803 804 805 807 808 809 810 811 812 813 814 815 816 817 818 819\n",
      " 820 821 822 823 824]\n",
      "Test indices: [ 16  18  36  62  73  84  88  91 102 103 104 111 121 124 126 129 136 139\n",
      " 157 178 181 184 194 198 206 212 226 227 237 238 241 246 254 267 268 274\n",
      " 275 295 301 303 311 313 337 340 342 378 383 386 405 406 433 439 450 458\n",
      " 468 479 496 527 533 546 577 578 580 611 619 627 672 680 682 691 699 718\n",
      " 723 728 732 781 784 785 795 797 802 806]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 50)                10800     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,633\n",
      "Trainable params: 11,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "Train on 631 samples, validate on 112 samples\n",
      "Epoch 1/20\n",
      "631/631 [==============================] - 12s 20ms/step - loss: 0.6937 - binary_accuracy: 0.4929 - val_loss: 0.7077 - val_binary_accuracy: 0.3393\n",
      "Epoch 2/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6916 - binary_accuracy: 0.5103 - val_loss: 0.6972 - val_binary_accuracy: 0.3661\n",
      "Epoch 3/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6886 - binary_accuracy: 0.5246 - val_loss: 0.7153 - val_binary_accuracy: 0.3661\n",
      "Epoch 4/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6891 - binary_accuracy: 0.5531 - val_loss: 0.6826 - val_binary_accuracy: 0.6518\n",
      "Epoch 5/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6856 - binary_accuracy: 0.5452 - val_loss: 0.6991 - val_binary_accuracy: 0.4375\n",
      "Epoch 6/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6781 - binary_accuracy: 0.5816 - val_loss: 0.6273 - val_binary_accuracy: 0.6875\n",
      "Epoch 7/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6908 - binary_accuracy: 0.5436 - val_loss: 0.6811 - val_binary_accuracy: 0.6429\n",
      "Epoch 8/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6835 - binary_accuracy: 0.5578 - val_loss: 0.6823 - val_binary_accuracy: 0.5893\n",
      "Epoch 9/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6799 - binary_accuracy: 0.6086 - val_loss: 0.6793 - val_binary_accuracy: 0.5714\n",
      "Epoch 00009: early stopping\n",
      "Accuracy is 0.6585365853658537\n",
      "AUC is 0.756578947368421\n",
      "Train indices: [  0   2   3   4   5   6   7   9  10  12  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
      "  39  40  41  43  44  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  65  66  68  70  71  72  73  74  75  76  79  80\n",
      "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
      " 100 101 102 103 104 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
      " 137 138 139 140 141 142 145 146 147 148 149 150 151 152 153 154 155 157\n",
      " 158 159 162 163 164 165 166 167 168 169 170 171 172 173 174 175 177 178\n",
      " 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196\n",
      " 197 198 199 200 201 202 203 204 205 206 207 209 211 212 213 214 215 216\n",
      " 217 218 219 220 221 222 223 224 225 226 227 228 229 230 232 233 234 235\n",
      " 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253\n",
      " 254 256 257 258 259 260 261 262 263 264 267 268 270 271 272 273 274 275\n",
      " 276 277 278 279 280 281 282 283 284 285 286 288 289 290 291 292 293 294\n",
      " 295 296 297 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313\n",
      " 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331\n",
      " 332 333 334 335 336 337 339 340 342 343 344 345 346 347 348 349 350 351\n",
      " 352 353 354 355 356 357 358 359 360 361 362 364 365 366 367 368 370 371\n",
      " 372 373 374 375 376 377 378 379 381 382 383 384 385 386 387 388 389 390\n",
      " 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408\n",
      " 409 411 412 413 414 415 417 418 419 420 421 422 423 424 425 426 429 430\n",
      " 431 432 433 434 435 437 438 439 441 442 443 444 445 446 447 448 449 450\n",
      " 451 452 453 454 455 456 457 458 459 460 462 463 464 465 466 467 468 469\n",
      " 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487\n",
      " 488 489 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 507\n",
      " 508 509 510 511 512 513 514 515 516 518 522 523 524 525 526 527 528 529\n",
      " 530 531 532 533 534 535 537 538 540 541 542 543 544 545 546 547 549 550\n",
      " 551 552 553 554 555 556 557 558 562 563 564 565 566 567 568 569 570 571\n",
      " 572 574 575 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591\n",
      " 592 593 594 595 596 597 598 599 600 601 602 603 604 606 607 608 609 610\n",
      " 611 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 630 631\n",
      " 632 633 635 636 637 638 639 640 641 642 643 644 645 646 647 648 650 651\n",
      " 652 653 654 655 656 657 660 661 662 663 665 666 668 669 670 672 673 674\n",
      " 675 676 677 678 679 680 682 683 684 685 686 687 688 689 691 692 693 694\n",
      " 695 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 713 715\n",
      " 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 734 735\n",
      " 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 754\n",
      " 755 757 758 759 760 761 762 763 764 765 766 767 769 770 771 772 775 776\n",
      " 777 778 779 780 781 782 783 784 785 786 787 788 789 791 792 793 794 795\n",
      " 796 797 798 799 800 801 802 803 804 805 806 808 809 810 812 813 814 816\n",
      " 820 821 822 823 824]\n",
      "Test indices: [  1   8  11  42  45  67  69  77  78  95 105 143 144 156 160 161 176 208\n",
      " 210 231 255 265 266 269 287 298 338 341 363 369 380 410 416 427 428 436\n",
      " 440 461 490 506 517 519 520 521 536 539 548 559 560 561 573 576 605 612\n",
      " 628 629 634 649 658 659 664 667 671 681 690 696 712 714 716 733 753 756\n",
      " 768 773 774 790 807 811 815 817 818 819]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_19 (LSTM)               (None, 50)                10800     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,633\n",
      "Trainable params: 11,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 631 samples, validate on 112 samples\n",
      "Epoch 1/20\n",
      "631/631 [==============================] - 13s 20ms/step - loss: 0.6923 - binary_accuracy: 0.4960 - val_loss: 0.7212 - val_binary_accuracy: 0.2857\n",
      "Epoch 2/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6920 - binary_accuracy: 0.5261 - val_loss: 0.7187 - val_binary_accuracy: 0.2857\n",
      "Epoch 3/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6879 - binary_accuracy: 0.5309 - val_loss: 0.6988 - val_binary_accuracy: 0.4107\n",
      "Epoch 4/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6945 - binary_accuracy: 0.5214 - val_loss: 0.6733 - val_binary_accuracy: 0.7500\n",
      "Epoch 5/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6846 - binary_accuracy: 0.5626 - val_loss: 0.7054 - val_binary_accuracy: 0.3839\n",
      "Epoch 6/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6821 - binary_accuracy: 0.5594 - val_loss: 0.7021 - val_binary_accuracy: 0.4464\n",
      "Epoch 7/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6788 - binary_accuracy: 0.5563 - val_loss: 0.7019 - val_binary_accuracy: 0.4196\n",
      "Epoch 00007: early stopping\n",
      "Accuracy is 0.4268292682926829\n",
      "AUC is 0.5884498480243161\n",
      "Train indices: [  0   1   2   3   4   6   7   8   9  10  11  13  14  15  16  17  18  19\n",
      "  20  21  22  23  24  25  26  27  28  30  31  32  33  34  36  37  38  40\n",
      "  41  42  43  44  45  46  48  50  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  77  78  79\n",
      "  80  81  82  83  84  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
      "  99 101 102 103 104 105 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 132 133 134 135 136 137 138\n",
      " 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156\n",
      " 157 158 159 160 161 162 163 164 165 166 167 168 170 171 172 173 174 175\n",
      " 176 177 178 179 181 183 184 185 186 187 188 189 191 192 193 194 195 196\n",
      " 198 199 200 201 202 204 205 206 207 208 209 210 211 212 213 214 216 217\n",
      " 218 219 220 221 222 223 224 226 227 228 229 231 232 235 236 237 238 239\n",
      " 240 241 242 243 244 245 246 248 249 250 251 252 253 254 255 256 257 258\n",
      " 259 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 277 278\n",
      " 279 280 281 283 284 285 286 287 289 290 291 292 293 294 295 296 297 298\n",
      " 299 300 301 302 303 304 305 306 307 308 310 311 312 313 314 316 317 318\n",
      " 319 320 321 322 323 324 325 326 327 328 329 330 333 334 335 336 337 338\n",
      " 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356\n",
      " 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 374 375\n",
      " 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 394\n",
      " 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412\n",
      " 414 415 416 417 418 419 420 422 423 424 425 426 427 428 429 430 431 432\n",
      " 433 435 436 437 439 440 441 442 444 445 446 447 448 449 450 451 452 453\n",
      " 454 455 457 458 459 461 462 463 464 465 466 467 468 470 471 472 473 477\n",
      " 478 479 480 481 482 483 484 485 487 489 490 491 492 493 494 496 498 499\n",
      " 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517\n",
      " 518 519 520 521 522 523 525 526 527 528 530 531 532 533 534 535 536 537\n",
      " 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 556\n",
      " 557 558 559 560 561 562 563 564 565 567 568 569 570 571 572 573 574 576\n",
      " 577 578 579 580 581 582 583 584 585 586 587 590 591 592 593 594 595 596\n",
      " 597 598 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615\n",
      " 616 618 619 620 621 623 624 625 626 627 628 629 630 631 632 634 635 637\n",
      " 638 639 640 641 642 643 644 645 649 650 651 652 653 655 656 657 658 659\n",
      " 660 661 662 663 664 665 666 667 668 669 671 672 673 674 676 677 679 680\n",
      " 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698\n",
      " 699 700 701 702 703 704 705 706 707 708 709 711 712 714 715 716 717 718\n",
      " 719 720 721 722 723 724 725 727 728 730 731 732 733 734 736 737 739 740\n",
      " 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 759\n",
      " 760 762 763 764 765 766 767 768 769 770 771 772 773 774 776 778 779 780\n",
      " 781 782 783 784 785 786 787 788 789 790 791 792 793 795 796 797 799 800\n",
      " 801 802 803 804 805 806 807 808 810 811 812 813 814 815 816 817 818 819\n",
      " 820 821 822 823 824]\n",
      "Test indices: [  5  12  29  35  39  47  49  76  85 100 106 107 131 169 180 182 190 197\n",
      " 203 215 225 230 233 234 247 260 276 282 288 309 315 331 332 373 393 413\n",
      " 421 434 438 443 456 460 469 474 475 476 486 488 495 497 524 529 555 566\n",
      " 575 588 589 599 617 622 633 636 646 647 648 654 670 675 678 710 713 726\n",
      " 729 735 738 758 761 775 777 794 798 809]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 50)                10800     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,633\n",
      "Trainable params: 11,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "Train on 631 samples, validate on 112 samples\n",
      "Epoch 1/20\n",
      "631/631 [==============================] - 13s 20ms/step - loss: 0.6933 - binary_accuracy: 0.4818 - val_loss: 0.7037 - val_binary_accuracy: 0.3214\n",
      "Epoch 2/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6910 - binary_accuracy: 0.5040 - val_loss: 0.6917 - val_binary_accuracy: 0.4911\n",
      "Epoch 3/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6901 - binary_accuracy: 0.5325 - val_loss: 0.6570 - val_binary_accuracy: 0.6786\n",
      "Epoch 4/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6947 - binary_accuracy: 0.5008 - val_loss: 0.6788 - val_binary_accuracy: 0.6518\n",
      "Epoch 5/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6913 - binary_accuracy: 0.5277 - val_loss: 0.6842 - val_binary_accuracy: 0.6429\n",
      "Epoch 6/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6895 - binary_accuracy: 0.5404 - val_loss: 0.6965 - val_binary_accuracy: 0.3661\n",
      "Epoch 00006: early stopping\n",
      "Accuracy is 0.5121951219512195\n",
      "AUC is 0.5930993456276026\n",
      "Train indices: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  20  22  23  24  25  26  27  28  29  30  31  32  34  35  36  37  38  39\n",
      "  40  41  42  43  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  65  66  67  69  70  71  73  74  76  77  78  79\n",
      "  80  81  82  84  85  86  87  88  89  90  91  93  94  95  96  97  98  99\n",
      " 100 101 102 103 104 105 106 107 108 109 110 111 113 114 115 116 117 118\n",
      " 119 120 121 122 124 125 126 127 128 129 131 132 133 134 135 136 138 139\n",
      " 140 141 142 143 144 145 146 147 148 149 150 153 154 155 156 157 158 159\n",
      " 160 161 162 163 164 165 166 168 169 170 171 172 173 174 176 177 178 179\n",
      " 180 181 182 184 186 187 188 190 191 192 193 194 197 198 199 200 202 203\n",
      " 204 205 206 207 208 209 210 212 213 214 215 216 217 218 220 221 222 223\n",
      " 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242\n",
      " 243 244 245 246 247 248 249 251 252 253 254 255 256 258 259 260 261 262\n",
      " 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 280 281\n",
      " 282 283 284 285 287 288 289 291 292 293 294 295 296 297 298 299 300 301\n",
      " 302 303 304 305 306 307 308 309 310 311 313 314 315 316 318 319 320 321\n",
      " 322 323 324 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 346 347 350 351 352 353 354 355 357 358 359 360 361 362\n",
      " 363 364 365 367 368 369 370 371 372 373 375 376 377 378 379 380 381 382\n",
      " 383 384 385 386 387 388 389 390 392 393 394 395 396 397 399 401 403 404\n",
      " 405 406 407 408 409 410 411 412 413 414 416 417 418 419 420 421 422 423\n",
      " 424 425 426 427 428 429 430 431 433 434 435 436 437 438 439 440 441 442\n",
      " 443 445 446 447 449 450 451 452 453 454 455 456 457 458 459 460 461 463\n",
      " 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 482\n",
      " 484 485 486 487 488 489 490 491 492 493 495 496 497 498 499 500 501 503\n",
      " 504 505 506 507 508 509 510 511 513 514 515 517 518 519 520 521 523 524\n",
      " 525 526 527 528 529 530 531 532 533 534 536 538 539 540 541 542 543 544\n",
      " 545 546 547 548 549 551 552 553 555 556 557 558 559 560 561 562 563 564\n",
      " 565 566 567 568 569 570 571 572 573 574 575 576 577 578 580 581 582 585\n",
      " 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603\n",
      " 605 606 607 608 609 610 611 612 613 614 616 617 618 619 620 621 622 623\n",
      " 624 625 626 627 628 629 630 632 633 634 635 636 637 638 639 640 641 642\n",
      " 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660\n",
      " 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679\n",
      " 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 698\n",
      " 699 700 701 702 705 706 707 708 709 710 711 712 713 714 715 716 717 718\n",
      " 719 720 721 722 723 724 725 726 727 728 729 731 732 733 734 735 736 737\n",
      " 738 739 740 742 743 744 746 747 748 749 750 751 752 753 754 755 756 757\n",
      " 758 760 761 762 765 766 767 768 769 770 771 773 774 775 776 777 778 779\n",
      " 780 781 782 783 784 785 786 788 789 790 791 793 794 795 796 797 798 799\n",
      " 800 801 802 803 804 806 807 808 809 810 811 812 813 814 815 816 817 818\n",
      " 819 821 822 823 824]\n",
      "Test indices: [  6  19  21  33  44  68  72  75  83  92 112 123 130 137 151 152 167 175\n",
      " 183 185 189 195 196 201 211 219 224 250 257 279 286 290 312 317 325 345\n",
      " 348 349 356 366 374 391 398 400 402 415 432 444 448 462 481 483 494 502\n",
      " 512 516 522 535 537 550 554 579 583 584 604 615 631 661 697 703 704 730\n",
      " 741 745 759 763 764 772 787 792 805 820]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 50)                10800     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,633\n",
      "Trainable params: 11,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 631 samples, validate on 112 samples\n",
      "Epoch 1/20\n",
      "631/631 [==============================] - 13s 20ms/step - loss: 0.6937 - binary_accuracy: 0.5040 - val_loss: 0.7075 - val_binary_accuracy: 0.3214\n",
      "Epoch 2/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6914 - binary_accuracy: 0.5182 - val_loss: 0.6899 - val_binary_accuracy: 0.5536\n",
      "Epoch 3/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6903 - binary_accuracy: 0.5341 - val_loss: 0.6499 - val_binary_accuracy: 0.6786\n",
      "Epoch 4/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6886 - binary_accuracy: 0.5357 - val_loss: 0.6795 - val_binary_accuracy: 0.7143\n",
      "Epoch 5/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6851 - binary_accuracy: 0.6117 - val_loss: 0.6901 - val_binary_accuracy: 0.5357\n",
      "Epoch 6/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6778 - binary_accuracy: 0.5626 - val_loss: 0.6149 - val_binary_accuracy: 0.6786\n",
      "Epoch 7/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6981 - binary_accuracy: 0.5246 - val_loss: 0.6805 - val_binary_accuracy: 0.6518\n",
      "Epoch 8/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6880 - binary_accuracy: 0.5499 - val_loss: 0.6886 - val_binary_accuracy: 0.6071\n",
      "Epoch 9/20\n",
      "631/631 [==============================] - 10s 15ms/step - loss: 0.6881 - binary_accuracy: 0.5800 - val_loss: 0.6805 - val_binary_accuracy: 0.6250\n",
      "Epoch 00009: early stopping\n",
      "Accuracy is 0.524390243902439\n",
      "AUC is 0.5849849849849851\n",
      "0.6572549249593426\n",
      "0.5283573317660888\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "acc_list = []\n",
    "AUC_list = []\n",
    "# f1_list = []\n",
    "\n",
    "np.random.seed(14)  # fix the random numbers generator state\n",
    "\n",
    "batch_size = 16\n",
    "hidden_units = 50\n",
    "input_shape = X_train.shape[1:]\n",
    "nb_epochs = 20\n",
    "nb_classes = 1\n",
    "dropout = 0.05\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3, verbose=1)\n",
    "print('Build model...')\n",
    "\n",
    "# sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "for train, test in kfold.split(X_train, Y_train):\n",
    "    print('Train indices:', train)\n",
    "    print('Test indices:', test)\n",
    "    \n",
    "#     train_set = np.array((0, 0, 0))\n",
    "#     target_set = np.array(0)\n",
    "#     X_train, Y_train = merge_df_rows(df.iloc[train])\n",
    "    \n",
    "#     train_set = np.array((0, 0, 0))\n",
    "#     target_set = np.array(0)\n",
    "#     X_test, Y_test = merge_df_rows(df.iloc[test])\n",
    "    \n",
    "    input_shape = X_train.shape[1:]\n",
    "\n",
    "    model = Sequential()\n",
    "#     model.add(Bidirectional(LSTM(units=hidden_units, kernel_initializer='uniform', recurrent_initializer='uniform', \n",
    "#                    dropout=dropout, use_bias=True, unit_forget_bias=True, activation='tanh', recurrent_activation='sigmoid', \n",
    "#                    input_shape=input_shape), input_shape=input_shape, merge_mode='concat'))\n",
    "    model.add(LSTM(units=hidden_units, kernel_initializer='uniform', recurrent_initializer='uniform', \n",
    "                   dropout=dropout, use_bias=True, unit_forget_bias=True, activation='tanh', recurrent_activation='sigmoid', \n",
    "                   input_shape=input_shape))\n",
    "\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', metrics=['binary_accuracy'], optimizer='adam')\n",
    "\n",
    "    print(\"Train...\")\n",
    "    history = model.fit(X_train[train], Y_train[train], batch_size=batch_size, epochs=nb_epochs, verbose=1,\n",
    "                        callbacks=[early_stopping], validation_split=0.15)\n",
    "\n",
    "    Y_pred = model.predict_proba(X_train[test])\n",
    "    acc = accuracy_score(Y_train[test], np.round(Y_pred))\n",
    "    print('Accuracy is', acc)\n",
    "    AUC = roc_auc_score(Y_train[test], Y_pred)\n",
    "    print('AUC is', AUC)\n",
    "#     f1 = f1_score(Y_train[test], np.round(Y_pred))\n",
    "#     print('F1-score is', f1)\n",
    "    \n",
    "    acc_list.append(acc)\n",
    "    AUC_list.append(AUC)\n",
    "#     f1_list.append(f1)\n",
    "\n",
    "print(np.mean(AUC_list))\n",
    "print(np.mean(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7604895104895104,\n",
       " 0.5900360144057623,\n",
       " 0.7279761904761904,\n",
       " 0.5932400932400932,\n",
       " 0.659698025551684,\n",
       " 0.7179962894248608,\n",
       " 0.756578947368421,\n",
       " 0.5884498480243161,\n",
       " 0.5930993456276026,\n",
       " 0.5849849849849851]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
